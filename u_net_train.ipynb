{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tsp_tinker_utils import TSPPackage, get_tsp_problem_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getenv('DATA_PATH', './data')\n",
    "BSSF_PATH = os.getenv('BSSF_PATH', './bssf')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "VALIDATION_PROP = 0.1\n",
    "TRAIN_PROBLEM_SIZE_CUTOFF = 60\n",
    "TEST_PROBLEM_SIZE_CUTOFF = 201\n",
    "LR = 0.001\n",
    "NUM_PATH_VARIATIONS_PER_EXAMPLE = 8\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_path_variations_from_problem(city_connections_w_costs: np.ndarray, edge_matrix: np.ndarray, edges: List[Tuple[int, int]], num_samples: int) -> Tuple[List[List[Tuple[int, int]]], np.ndarray, np.ndarray]:\n",
    "    num_cities = city_connections_w_costs.shape[0]\n",
    "    \n",
    "    batch_selected_edges = []\n",
    "    num_edges_lb = 0\n",
    "    num_edges_ub = num_cities - 2\n",
    "    for i in range(num_samples):\n",
    "        num_edges_to_include = random.randint(num_edges_lb, num_edges_ub)\n",
    "        direction = bool(random.randint(0, 1))\n",
    "\n",
    "        if num_edges_to_include == 0:\n",
    "            num_edges_lb = 1\n",
    "\n",
    "        random.shuffle(edges)\n",
    "        if direction:\n",
    "            batch_selected_edges.append(edges[:num_edges_to_include])\n",
    "        else:\n",
    "            reversed_edges = [(out_c, in_c) for in_c, out_c in edges[:num_edges_to_include]]\n",
    "            batch_selected_edges.append(reversed_edges)\n",
    "\n",
    "    target = edge_matrix\n",
    "\n",
    "    # return batch, target, city_connections_w_costs\n",
    "    target = np.tile(target, (num_samples, 1, 1))\n",
    "\n",
    "    batch = np.zeros((num_samples, 2, num_cities, num_cities), dtype=np.float32)\n",
    "    for i, path in enumerate(batch_selected_edges):\n",
    "        for in_c, out_c in path:\n",
    "            batch[i, 1, in_c, out_c] = 1\n",
    "\n",
    "        batch[i, 0] = city_connections_w_costs\n",
    "\n",
    "    return batch, target\n",
    "\n",
    "def load_and_process_tsp_problem(file_path: os.PathLike) -> Tuple[List[List[Tuple[int, int]]], np.ndarray, np.ndarray]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    packaged_problem = TSPPackage.from_json(json_data)\n",
    "    problem = packaged_problem.problem\n",
    "    best_solution = packaged_problem.best_solution\n",
    "\n",
    "    # Convert the path to edge matrix\n",
    "    edge_matrix = np.zeros((problem.num_cities, problem.num_cities), dtype=np.float32)\n",
    "    edges = []\n",
    "    previous_city = 0\n",
    "    for city in best_solution.tour:\n",
    "        edge_matrix[previous_city, city] = 1\n",
    "        edges.append((previous_city, city))\n",
    "        previous_city = city\n",
    "\n",
    "    return sample_path_variations_from_problem(problem.city_connections_w_costs, edge_matrix, edges, NUM_PATH_VARIATIONS_PER_EXAMPLE)\n",
    "    \n",
    "\n",
    "class TSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: List[Tuple[torch.Tensor, torch.Tensor]], num_path_variations: int = NUM_PATH_VARIATIONS_PER_EXAMPLE):\n",
    "        super(TSPDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.num_path_variations = num_path_variations\n",
    "\n",
    "    @classmethod\n",
    "    def from_disk(cls, data_folder_path: os.PathLike, problem_size_lower_bound: Optional[int] = None, problem_size_upper_bound: Optional[int] = None, undirected_only: Optional[bool] = True, max_workers: int = cpu_count() - 1):\n",
    "        new_instance = cls([])\n",
    "        new_instance.data = new_instance._multi_threaded_load(data_folder_path, problem_size_lower_bound, problem_size_upper_bound, max_workers, undirected_only)\n",
    "\n",
    "        return new_instance\n",
    "    \n",
    "    def _multi_threaded_load(self, data_folder_path: os.PathLike, problem_size_lower_bound: Optional[int], problem_size_upper_bound: Optional[int], max_workers: int, undirected_only: Optional[bool] = True):\n",
    "        possible_folders = get_tsp_problem_folders(data_folder_path)\n",
    "        problem_file_paths = []\n",
    "        for _, problem_size, folder_path in possible_folders:\n",
    "            if (problem_size_lower_bound is None or problem_size >= problem_size_lower_bound) and (problem_size_upper_bound is None or problem_size <= problem_size_upper_bound):\n",
    "                # Add all files in the folder to the list of files to load\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if file.endswith('.json'):\n",
    "                        problem_file_paths.append(os.path.join(folder_path, file))\n",
    "\n",
    "        formatted_data = []\n",
    "        with Pool(processes=max_workers) as worker_pool:\n",
    "            with tqdm(total=len(problem_file_paths), desc=\"Loading data from disk...\") as p_bar:\n",
    "                for result in worker_pool.imap_unordered(load_and_process_tsp_problem, problem_file_paths, chunksize=10):\n",
    "                    p_bar.update(1)\n",
    "                    formatted_data.append(result)\n",
    "\n",
    "        return formatted_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path_variation_edges, cost_matrix, edge_matrix =  self.data[idx]\n",
    "\n",
    "        # target = torch.tensor(edge_matrix, dtype=torch.float32)\n",
    "        # target = target.repeat(self.num_path_variations, 1, 1)\n",
    "\n",
    "        # batch = np.zeros((self.num_path_variations, 2, cost_matrix.shape[0], cost_matrix.shape[1]))\n",
    "        # for i, path in enumerate(path_variation_edges):\n",
    "        #     for in_c, out_c in path:\n",
    "        #         batch[i, 1, in_c, out_c] = 1\n",
    "\n",
    "        #     batch[i, 0] = cost_matrix\n",
    "\n",
    "        # return torch.tensor(batch, dtype=torch.float32), target\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def split(self, ration: float = 0.1):\n",
    "        num_to_take = int(len(self) * ration)\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "        split_data = self.data[:num_to_take]\n",
    "        self.data = self.data[num_to_take:]\n",
    "\n",
    "        return TSPDataset(split_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalSalesmanNet(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 5, padding: int = 2):\n",
    "        super(ConvolutionalSalesmanNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 4, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4, 8, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 10 layers down\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 20 layers down, head back up\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 10 layers up, 10 layers to go\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4, 2, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa08bc033f64c17b91938ab05fb08fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data from disk...:   0%|          | 0/138000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = TSPDataset.from_disk(DATA_PATH, problem_size_upper_bound=TRAIN_PROBLEM_SIZE_CUTOFF)\n",
    "validation_dataset = train_dataset.split(VALIDATION_PROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalSalesmanNet().to(DEVICE)\n",
    "loss_fn = nn.MSELoss().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), BSSF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bssf_validation = 465654654566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f8691bfaa4c46ad7bab092a947ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/124200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'float' object is not subscriptable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Dang memory leaks\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m tot_validation_loss \u001b[38;5;241m/\u001b[39m num_validation_batches\n\u001b[1;32m     46\u001b[0m validation_losses\u001b[38;5;241m.\u001b[39mappend(validation_loss)\n\u001b[0;32m---> 47\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix(train_loss\u001b[38;5;241m=\u001b[39m train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], validation_loss\u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_loss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_loss \u001b[38;5;241m<\u001b[39m bssf_validation:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew BSSF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbssf_validation\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mvalidation_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m improvement!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_train_batches = len(train_loader)\n",
    "    num_validation_batches = len(validation_loader)\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        tot_train_loss = 0\n",
    "        train_bar = tqdm(range(num_train_batches), desc=f\"Epoch {i+1}/{NUM_EPOCHS}\")\n",
    "        for j, (batch, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Format the data and move to GPU\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            target = target[0].to(DEVICE)\n",
    "\n",
    "            # Forward pass and adjust\n",
    "            path_predictions: torch.Tensor = model(batch)\n",
    "            loss: torch.Tensor = loss_fn(path_predictions, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tot_train_loss += loss.item()\n",
    "            train_bar.update(1)\n",
    "\n",
    "\n",
    "            if j % 100 == 0:\n",
    "                train_bar.set_postfix(train_loss= tot_train_loss / (j + 1))\n",
    "\n",
    "        train_losses.append(tot_train_loss / num_train_batches)\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        tot_validation_loss = 0\n",
    "        for j, (batch, target) in enumerate(validation_loader):\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            target = target[0].to(DEVICE)\n",
    "\n",
    "            path_predictions = model(batch)\n",
    "            loss = loss_fn(path_predictions, target)\n",
    "\n",
    "            tot_validation_loss += loss.item()\n",
    "\n",
    "        validation_loss = tot_validation_loss / num_validation_batches\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_bar.set_postfix(train_loss= train_losses[-1], validation_loss= validation_losses[-1])\n",
    "\n",
    "        if validation_loss < bssf_validation:\n",
    "            print(f\"New BSSF: {validation_loss} ----> {bssf_validation - validation_loss} improvement!\")\n",
    "            bssf_validation = validation_loss\n",
    "            torch.save(model.state_dict(), BSSF_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    # Dang memory leaks\n",
    "    del model\n",
    "\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TSPDataset(DATA_PATH, problem_size_lower_bound=TEST_PROBLEM_SIZE_CUTOFF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
