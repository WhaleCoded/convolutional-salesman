{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tsp_tinker_utils import TSPPackage, get_tsp_problem_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getenv('DATA_PATH', './data')\n",
    "BSSF_PATH = os.getenv('BSSF_PATH', './bssf')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "VALIDATION_PROP = 0.1\n",
    "TRAIN_PROBLEM_SIZE_CUTOFF = 100\n",
    "TEST_PROBLEM_SIZE_CUTOFF = 201\n",
    "LR = 0.001\n",
    "NUM_PATH_VARIATIONS_PER_EXAMPLE = 8\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_path_variations_from_problem(city_connections_w_costs: np.ndarray, edge_matrix: np.ndarray, edges: List[Tuple[int, int]], num_samples: int) -> Tuple[List[List[Tuple[int, int]]], np.ndarray, np.ndarray]:\n",
    "    num_cities = city_connections_w_costs.shape[0]\n",
    "    \n",
    "    batch = []\n",
    "    num_edges_lb = 0\n",
    "    num_edges_ub = num_cities - 2\n",
    "    for i in range(num_samples):\n",
    "        num_edges_to_include = random.randint(num_edges_lb, num_edges_ub)\n",
    "        direction = bool(random.randint(0, 1))\n",
    "\n",
    "        if num_edges_to_include == 0:\n",
    "            num_edges_lb = 1\n",
    "\n",
    "        random.shuffle(edges)\n",
    "        if direction:\n",
    "            batch.append(edges[:num_edges_to_include])\n",
    "        else:\n",
    "            reversed_edges = [(out_c, in_c) for in_c, out_c in edges[:num_edges_to_include]]\n",
    "            batch.append(reversed_edges)\n",
    "\n",
    "    target = edge_matrix\n",
    "\n",
    "    return batch, target, city_connections_w_costs\n",
    "\n",
    "def load_and_process_tsp_problem(file_path: os.PathLike) -> Tuple[List[List[Tuple[int, int]]], np.ndarray, np.ndarray]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    packaged_problem = TSPPackage.from_json(json_data)\n",
    "    problem = packaged_problem.problem\n",
    "    best_solution = packaged_problem.best_solution\n",
    "\n",
    "    # Convert the path to edge matrix\n",
    "    edge_matrix = np.zeros((problem.num_cities, problem.num_cities))\n",
    "    edges = []\n",
    "    previous_city = 0\n",
    "    for city in best_solution.tour:\n",
    "        edge_matrix[previous_city, city] = 1\n",
    "        edges.append((previous_city, city))\n",
    "        previous_city = city\n",
    "\n",
    "    return sample_path_variations_from_problem(problem.city_connections_w_costs, edge_matrix, edges, NUM_PATH_VARIATIONS_PER_EXAMPLE)\n",
    "    \n",
    "\n",
    "class TSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: List[Tuple[torch.Tensor, torch.Tensor]], num_path_variations: int = NUM_PATH_VARIATIONS_PER_EXAMPLE):\n",
    "        super(TSPDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.num_path_variations = num_path_variations\n",
    "\n",
    "    @classmethod\n",
    "    def from_disk(cls, data_folder_path: os.PathLike, problem_size_lower_bound: Optional[int] = None, problem_size_upper_bound: Optional[int] = None, undirected_only: Optional[bool] = True, max_workers: int = cpu_count() - 1):\n",
    "        new_instance = cls([])\n",
    "        new_instance.data = new_instance._multi_threaded_load(data_folder_path, problem_size_lower_bound, problem_size_upper_bound, max_workers, undirected_only)\n",
    "\n",
    "        return new_instance\n",
    "    \n",
    "    def _multi_threaded_load(self, data_folder_path: os.PathLike, problem_size_lower_bound: Optional[int], problem_size_upper_bound: Optional[int], max_workers: int, undirected_only: Optional[bool] = True):\n",
    "        possible_folders = get_tsp_problem_folders(data_folder_path)\n",
    "        problem_file_paths = []\n",
    "        for _, problem_size, folder_path in possible_folders:\n",
    "            if (problem_size_lower_bound is None or problem_size >= problem_size_lower_bound) and (problem_size_upper_bound is None or problem_size <= problem_size_upper_bound):\n",
    "                # Add all files in the folder to the list of files to load\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if file.endswith('.json'):\n",
    "                        problem_file_paths.append(os.path.join(folder_path, file))\n",
    "\n",
    "        formatted_data = []\n",
    "        with Pool(processes=max_workers) as worker_pool:\n",
    "            with tqdm(total=len(problem_file_paths), desc=\"Loading data from disk...\") as p_bar:\n",
    "                for result in worker_pool.imap_unordered(load_and_process_tsp_problem, problem_file_paths, chunksize=10):\n",
    "                    p_bar.update(1)\n",
    "                    formatted_data.append(result)\n",
    "\n",
    "        return formatted_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_variation_edges, cost_matrix, edge_matrix =  self.data[idx]\n",
    "\n",
    "        target = torch.tensor(edge_matrix, dtype=torch.float32)\n",
    "        target = target.repeat(self.num_path_variations, 1, 1)\n",
    "\n",
    "        batch = np.zeros((self.num_path_variations, 2, cost_matrix.shape[0], cost_matrix.shape[1]))\n",
    "        for i, path in enumerate(path_variation_edges):\n",
    "            for in_c, out_c in path:\n",
    "                batch[i, 1, in_c, out_c] = 1\n",
    "\n",
    "            batch[i, 0] = cost_matrix\n",
    "\n",
    "        return torch.tensor(batch, dtype=torch.float32), target\n",
    "    \n",
    "    def split(self, ration: float = 0.1):\n",
    "        num_to_take = int(len(self) * ration)\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "        split_data = self.data[:num_to_take]\n",
    "        self.data = self.data[num_to_take:]\n",
    "\n",
    "        return TSPDataset(split_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalSalesmanNet(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 5, padding: int = 2):\n",
    "        super(ConvolutionalSalesmanNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 4, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4, 8, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 10 layers down\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 20 layers down, head back up\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            # 10 layers up, 10 layers to go\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(8, 4, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(4, 2, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_path_variations_from_problem(city_connections_w_costs: np.ndarray, tour_edges: np.ndarray, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#     num_cities = city_connections_w_costs.shape[0]\n",
    "#     edges = tour_edges.nonzero()\n",
    "#     edges = [(in_c, out_c) for in_c, out_c in zip(edges[0], edges[1])]\n",
    "\n",
    "    \n",
    "#     batch: np.ndarray = np.zeros((num_samples, 2, num_cities, num_cities))\n",
    "#     num_edges_lb = 0\n",
    "#     num_edges_ub = num_cities - 2\n",
    "#     for i in range(num_samples):\n",
    "#         num_edges_to_include = random.randint(num_edges_lb, num_edges_ub)\n",
    "#         direction = bool(random.randint(0, 1))\n",
    "\n",
    "#         if num_edges_to_include == 0:\n",
    "#             # only one possible variation, don't double dip\n",
    "#             num_edges_lb = 1\n",
    "\n",
    "#         random.shuffle(edges)\n",
    "#         for in_c, out_c in edges[:num_edges_to_include]:\n",
    "#             if direction:\n",
    "#                 batch[i, 1, in_c, out_c] = 1\n",
    "#             else:\n",
    "#                 batch[i, 1, out_c, in_c] = 1\n",
    "#         batch[i, 0] = city_connections_w_costs\n",
    "\n",
    "#     target = torch.tensor(tour_edges, dtype=torch.long)\n",
    "#     target = target.repeat(num_samples, 1, 1)\n",
    "\n",
    "#     return torch.tensor(np.array(batch), dtype=torch.float32), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90edc20a8f304ea0acee039253481d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data from disk...:   0%|          | 0/218000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = TSPDataset.from_disk(DATA_PATH, problem_size_upper_bound=TRAIN_PROBLEM_SIZE_CUTOFF)\n",
    "validation_dataset = train_dataset.split(VALIDATION_PROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalSalesmanNet().to(DEVICE)\n",
    "loss_fn = nn.MSELoss().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bssf_validation = 465654654566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0614da35b3e3426f9bcce140ef0c5549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/196200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    num_train_batches = len(train_loader)\n",
    "    num_validation_batches = len(validation_loader)\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        tot_train_loss = 0\n",
    "        train_bar = tqdm(range(num_train_batches), desc=f\"Epoch {i+1}/{NUM_EPOCHS}\")\n",
    "        for j, (batch, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Format the data and move to GPU\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            target = target[0].to(DEVICE)\n",
    "\n",
    "            # Forward pass and adjust\n",
    "            path_predictions: torch.Tensor = model(batch)\n",
    "            loss: torch.Tensor = loss_fn(path_predictions, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tot_train_loss += loss.item()\n",
    "            train_bar.update(1)\n",
    "\n",
    "\n",
    "            if j % 100 == 0:\n",
    "                train_bar.set_postfix(train_loss= tot_train_loss / (j + 1))\n",
    "\n",
    "        train_losses.append(tot_train_loss / num_train_batches)\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        tot_validation_loss = 0\n",
    "        for j, (batch, target) in enumerate(validation_loader):\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            target = target[0].to(DEVICE)\n",
    "\n",
    "            path_predictions = model(batch)\n",
    "            loss = loss_fn(path_predictions, target)\n",
    "\n",
    "            tot_validation_loss += loss.item()\n",
    "\n",
    "        validation_loss = tot_validation_loss / num_validation_batches\n",
    "        validation_losses.append(validation_loss)\n",
    "        train_bar.set_postfix(train_loss= train_losses[-1], validation_loss= validation_loss[-1])\n",
    "\n",
    "        if validation_loss < bssf_validation:\n",
    "            print(f\"New BSSF: {validation_loss} ----> {bssf_validation - validation_loss} improvement!\")\n",
    "            bssf_validation = validation_loss\n",
    "            torch.save(model.state_dict(), BSSF_PATH)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    # Dang memory leaks\n",
    "    del model\n",
    "\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TSPDataset(DATA_PATH, problem_size_lower_bound=TEST_PROBLEM_SIZE_CUTOFF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
