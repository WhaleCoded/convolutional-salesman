{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data import TSPDataset, _normalize_np_array\n",
    "from config import Config, Checkpoint, MetaData, Metrics\n",
    "from model import ConvolutionalSalesmanNet, construct_path, calc_path_metric, calc_path_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.getenv(\"CONFIG_PATH\", \"./config.json\")\n",
    "\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    config = Config.from_json(CONFIG_PATH)\n",
    "else:\n",
    "    config = Config()\n",
    "    config.store_as_json(CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57566fbe811d4d379136c6ab89688c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data from disk...:   0%|          | 0/218000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 217000\n",
      "Validation dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TSPDataset.from_disk(config.data_path, config.num_path_variations_per_example, problem_size_upper_bound=config.train_problem_size_cutoff)\n",
    "\n",
    "validation_uuids = config.get_validation_uuids()\n",
    "if validation_uuids is None:\n",
    "    validation_dataset = train_dataset.stratified_split(config.validation_tot_size)\n",
    "    validation_uuids = validation_dataset.get_uuids()\n",
    "    config.store_validation_uuids(validation_uuids)\n",
    "else:\n",
    "    validation_dataset = train_dataset.split_by_uuids(validation_uuids)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_checkpoint = config.get_curr_checkpoint()\n",
    "bssf_path_const_metric = config.get_bssf_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint state dicts\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalSalesmanNet().to(config.device)\n",
    "loss_fn = nn.MSELoss().to(config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.min_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.max_lr, total_steps=config.tot_train_batches)\n",
    "\n",
    "if curr_checkpoint is None:\n",
    "    curr_checkpoint = Checkpoint(\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        Metrics(),\n",
    "        MetaData()\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading checkpoint state dicts\")\n",
    "    model.load_state_dict(curr_checkpoint.model_state_dict)\n",
    "    optimizer.load_state_dict(curr_checkpoint.optimizer_state_dict)\n",
    "    lr_scheduler.load_state_dict(curr_checkpoint.lr_scheduler_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84d88e94bee4274b3b63cb50c394731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted storing checkpoint gracefully...\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "master_p_bar = tqdm(range(config.tot_train_batches), desc=\"Training\")\n",
    "master_p_bar.update(curr_checkpoint.metadata.num_batches_trained)\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    curr_checkpoint_tot_loss = 0\n",
    "    while curr_checkpoint.metadata.num_batches_trained < config.tot_train_batches:\n",
    "        master_p_bar.set_description(\"Training\")\n",
    "        batch: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "\n",
    "        try:\n",
    "            (batch, target) = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            (batch, target) = next(train_iter)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Format the data and move to GPU\n",
    "        batch = batch.squeeze(0).to(config.device)\n",
    "        target = target.squeeze(0).to(config.device)\n",
    "\n",
    "        # Predict and Calculate Loss\n",
    "        path_predictions: torch.Tensor = model(batch)\n",
    "        loss: torch.Tensor = loss_fn(path_predictions, target)\n",
    "\n",
    "        # Adjust Model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Update Progress\n",
    "        curr_checkpoint.metadata.num_batches_trained += 1\n",
    "        master_p_bar.update(1)\n",
    "        curr_checkpoint_tot_loss += loss.item()\n",
    "\n",
    "        if curr_checkpoint.metadata.num_batches_trained % 100 == 0:\n",
    "            # Update postfix\n",
    "            num_batches_before_checkpoint =  curr_checkpoint.metadata.num_batches_trained % config.batches_per_checkpoint\n",
    "            trained_batches = config.batches_per_checkpoint - num_batches_before_checkpoint\n",
    "            master_p_bar.set_postfix(\n",
    "                train_loss= curr_checkpoint_tot_loss/ trained_batches, \n",
    "                vald_loss = curr_checkpoint.metrics.validation_loss[-1] if len(curr_checkpoint.metrics.validation_loss) > 0 else None,\n",
    "                path_construction_metric = curr_checkpoint.metrics.path_construction_metrics[-1] if len(curr_checkpoint.metrics.path_construction_metrics) > 0 else None)\n",
    "\n",
    "        if curr_checkpoint.metadata.num_batches_trained % config.batches_per_checkpoint == 0:\n",
    "            optimizer.zero_grad()\n",
    "            master_p_bar.set_description(\"Validating\")\n",
    "            # Run Validation and Save Checkpoint\n",
    "            curr_checkpoint.metrics.training_loss.append(curr_checkpoint_tot_loss / config.batches_per_checkpoint)\n",
    "            curr_checkpoint_tot_loss = 0\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tot_validation_loss = 0\n",
    "                path_construction_metric = []\n",
    "                for (batch, target) in validation_loader:\n",
    "                    batch = batch.squeeze(0).to(config.device)\n",
    "                    target = target.squeeze(0).to(config.device)\n",
    "\n",
    "                    path_predictions = model(batch)\n",
    "                    loss = loss_fn(path_predictions, target)\n",
    "\n",
    "                    tot_validation_loss += loss.item()\n",
    "\n",
    "                    constructed_path = construct_path(model, batch[0, 0])\n",
    "                    path_metric = calc_path_metric(constructed_path, target[0], batch[0, 0])\n",
    "                    path_construction_metric.append(path_metric)\n",
    "\n",
    "            curr_checkpoint.metrics.validation_loss.append(tot_validation_loss / len(validation_loader))\n",
    "            checkpoint_path_construction_average = np.mean(path_construction_metric)\n",
    "            curr_checkpoint.metrics.path_construction_metrics.append(checkpoint_path_construction_average)\n",
    "            curr_checkpoint.metrics.learning_rate.append(lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Update postfix\n",
    "            num_batches_before_checkpoint = config.batches_per_checkpoint % curr_checkpoint.metadata.num_batches_trained\n",
    "            trained_batches = config.batches_per_checkpoint - num_batches_before_checkpoint\n",
    "            master_p_bar.set_postfix(train_loss= curr_checkpoint.metrics.training_loss[-1], \n",
    "                                     vald_loss = curr_checkpoint.metrics.validation_loss[-1],\n",
    "                                     path_construction_metric = curr_checkpoint.metrics.path_construction_metrics[-1])\n",
    "\n",
    "\n",
    "            if bssf_path_const_metric is None or checkpoint_path_construction_average < bssf_path_const_metric:\n",
    "                bssf_path_const_metric = checkpoint_path_construction_average\n",
    "                config.store_new_bssf(model, curr_checkpoint.metrics)\n",
    "\n",
    "            # Get new state dicts\n",
    "            curr_checkpoint.model_state_dict = model.state_dict()\n",
    "            curr_checkpoint.optimizer_state_dict = optimizer.state_dict()\n",
    "            curr_checkpoint.lr_scheduler_state_dict = lr_scheduler.state_dict()\n",
    "            config.store_new_checkpoint(curr_checkpoint)\n",
    "            model.train()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted storing checkpoint gracefully...\")\n",
    "    curr_checkpoint.model_state_dict = model.state_dict()\n",
    "    curr_checkpoint.optimizer_state_dict = optimizer.state_dict()\n",
    "    curr_checkpoint.lr_scheduler_state_dict = lr_scheduler.state_dict()\n",
    "    config.store_new_checkpoint(curr_checkpoint)\n",
    "\n",
    "except Exception as e:\n",
    "    print(batch.shape)\n",
    "    print(e)\n",
    "\n",
    "    # Dang memory leaks\n",
    "    del model\n",
    "\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TSPDataset(DATA_PATH, problem_size_lower_bound=TEST_PROBLEM_SIZE_CUTOFF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
