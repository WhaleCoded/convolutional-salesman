{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data import TSPDataset\n",
    "from config import Config, Checkpoint, MetaData, Metrics\n",
    "from model import ConvolutionalSalesmanNet, construct_path, calc_path_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.getenv(\"CONFIG_PATH\", \"./config.json\")\n",
    "\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    config = Config.from_json(CONFIG_PATH)\n",
    "else:\n",
    "    config = Config()\n",
    "    config.store_as_json(CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc9b6034094466b39d59bb20edc542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data from disk...:   0%|          | 0/218000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 217000\n",
      "Validation dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TSPDataset.from_disk(config.data_path, config.num_path_variations_per_example, problem_size_upper_bound=config.train_problem_size_cutoff)\n",
    "\n",
    "validation_uuids = config.get_validation_uuids()\n",
    "if validation_uuids is None:\n",
    "    validation_dataset = train_dataset.stratified_split(config.validation_tot_size)\n",
    "    validation_uuids = validation_dataset.get_uuids()\n",
    "    config.store_validation_uuids(validation_uuids)\n",
    "else:\n",
    "    validation_dataset = train_dataset.split_by_uuids(validation_uuids)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_checkpoint = config.get_curr_checkpoint()\n",
    "bssf_path_const_metric = config.get_bssf_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint state dicts\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalSalesmanNet().to(config.device)\n",
    "loss_fn = nn.MSELoss().to(config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.min_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.max_lr, total_steps=config.tot_train_batches)\n",
    "\n",
    "if curr_checkpoint is None:\n",
    "    curr_checkpoint = Checkpoint(\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        Metrics(),\n",
    "        MetaData()\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading checkpoint state dicts\")\n",
    "    model.load_state_dict(curr_checkpoint.model_state_dict)\n",
    "    optimizer.load_state_dict(curr_checkpoint.optimizer_state_dict)\n",
    "    lr_scheduler.load_state_dict(curr_checkpoint.lr_scheduler_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a3249301964314a4b8b086206c82d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "master_p_bar = tqdm(range(config.tot_train_batches), desc=\"Training\")\n",
    "master_p_bar.update(curr_checkpoint.metadata.num_batches_trained)\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    curr_checkpoint_tot_loss = 0\n",
    "    while curr_checkpoint.metadata.num_batches_trained < config.tot_train_batches:\n",
    "        master_p_bar.set_description(\"Training\")\n",
    "        batch: torch.Tensor\n",
    "        target: torch.Tensor\n",
    "\n",
    "        try:\n",
    "            (batch, target) = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            (batch, target) = next(train_iter)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Format the data and move to GPU\n",
    "        batch = batch.squeeze(0).to(config.device)\n",
    "        target = target.squeeze(0).to(config.device)\n",
    "\n",
    "        # Predict and Calculate Loss\n",
    "        path_predictions: torch.Tensor = model(batch)\n",
    "        loss: torch.Tensor = loss_fn(path_predictions, target)\n",
    "\n",
    "        # Adjust Model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Update Progress\n",
    "        curr_checkpoint.metadata.num_batches_trained += 1\n",
    "        master_p_bar.update(1)\n",
    "        curr_checkpoint_tot_loss += loss.item()\n",
    "\n",
    "        if curr_checkpoint.metadata.num_batches_trained % 100 == 0:\n",
    "            # Update postfix\n",
    "            num_batches_before_checkpoint =  curr_checkpoint.metadata.num_batches_trained % config.batches_per_checkpoint\n",
    "            trained_batches = config.batches_per_checkpoint - num_batches_before_checkpoint\n",
    "            master_p_bar.set_postfix(\n",
    "                train_loss= curr_checkpoint_tot_loss/ trained_batches, \n",
    "                vald_loss = curr_checkpoint.metrics.validation_loss[-1] if len(curr_checkpoint.metrics.validation_loss) > 0 else None,\n",
    "                path_construction_metric = curr_checkpoint.metrics.path_construction_metrics[-1] if len(curr_checkpoint.metrics.path_construction_metrics) > 0 else None)\n",
    "\n",
    "        if curr_checkpoint.metadata.num_batches_trained % config.batches_per_checkpoint == 0 or curr_checkpoint.metadata.num_batches_trained == config.tot_train_batches:\n",
    "            optimizer.zero_grad()\n",
    "            master_p_bar.set_description(\"Validating\")\n",
    "            # Run Validation and Save Checkpoint\n",
    "            curr_checkpoint.metrics.training_loss.append(curr_checkpoint_tot_loss / config.batches_per_checkpoint)\n",
    "            curr_checkpoint_tot_loss = 0\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tot_validation_loss = 0\n",
    "                path_construction_metric = []\n",
    "                for (batch, target) in validation_loader:\n",
    "                    batch = batch.squeeze(0).to(config.device)\n",
    "                    target = target.squeeze(0).to(config.device)\n",
    "\n",
    "                    path_predictions = model(batch)\n",
    "                    loss = loss_fn(path_predictions, target)\n",
    "\n",
    "                    tot_validation_loss += loss.item()\n",
    "\n",
    "                    constructed_path = construct_path(model, batch[0, 0])\n",
    "                    path_metric = calc_path_metric(constructed_path, target[0], batch[0, 0])\n",
    "                    path_construction_metric.append(path_metric)\n",
    "\n",
    "            curr_checkpoint.metrics.validation_loss.append(tot_validation_loss / len(validation_loader))\n",
    "            checkpoint_path_construction_average = np.mean(path_construction_metric)\n",
    "            curr_checkpoint.metrics.path_construction_metrics.append(checkpoint_path_construction_average)\n",
    "            curr_checkpoint.metrics.learning_rate.append(lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Update postfix\n",
    "            num_batches_before_checkpoint = config.batches_per_checkpoint % curr_checkpoint.metadata.num_batches_trained\n",
    "            trained_batches = config.batches_per_checkpoint - num_batches_before_checkpoint\n",
    "            master_p_bar.set_postfix(train_loss= curr_checkpoint.metrics.training_loss[-1], \n",
    "                                     vald_loss = curr_checkpoint.metrics.validation_loss[-1],\n",
    "                                     path_construction_metric = curr_checkpoint.metrics.path_construction_metrics[-1])\n",
    "\n",
    "\n",
    "            if bssf_path_const_metric is None or checkpoint_path_construction_average < bssf_path_const_metric:\n",
    "                bssf_path_const_metric = checkpoint_path_construction_average\n",
    "                config.store_new_bssf(model, curr_checkpoint.metrics)\n",
    "\n",
    "            # Get new state dicts\n",
    "            curr_checkpoint.model_state_dict = model.state_dict()\n",
    "            curr_checkpoint.optimizer_state_dict = optimizer.state_dict()\n",
    "            curr_checkpoint.lr_scheduler_state_dict = lr_scheduler.state_dict()\n",
    "            config.store_new_checkpoint(curr_checkpoint)\n",
    "            model.train()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted storing checkpoint gracefully...\")\n",
    "    curr_checkpoint.model_state_dict = model.state_dict()\n",
    "    curr_checkpoint.optimizer_state_dict = optimizer.state_dict()\n",
    "    curr_checkpoint.lr_scheduler_state_dict = lr_scheduler.state_dict()\n",
    "    config.store_new_checkpoint(curr_checkpoint)\n",
    "\n",
    "except Exception as e:\n",
    "    print(batch.shape)\n",
    "    print(e)\n",
    "\n",
    "    # Dang memory leaks\n",
    "    del model\n",
    "\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b4d49752844b36b679862ce97c9344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Average Path Construction Time By Problem Size\n",
    "problem_size_construction_time = {}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tot_validation_loss = 0\n",
    "    path_construction_metric = []\n",
    "    for (batch, target) in tqdm(validation_loader):\n",
    "        batch = batch.squeeze(0).to(config.device)\n",
    "        target = target.squeeze(0).to(config.device)\n",
    "        problem_size = batch.shape[-1]\n",
    "\n",
    "        start = time.time()\n",
    "        constructed_path = construct_path(model, batch[0, 0])\n",
    "        end = time.time()\n",
    "\n",
    "        if problem_size not in problem_size_construction_time:\n",
    "            problem_size_construction_time[problem_size] = []\n",
    "        problem_size_construction_time[problem_size].append(end - start)\n",
    "\n",
    "# for size in problem_size_construction_time:\n",
    "#     problem_size_construction_time[size] = np.mean(problem_size_construction_time[size])\n",
    "\n",
    "# Store results in bssf_folder\n",
    "const_time_file_path = os.path.join(config.bssf_path, \"construction_time.json\")\n",
    "with open(const_time_file_path, \"w+\") as f:\n",
    "    json.dump(problem_size_construction_time, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset, train_loader, validation_dataset, validation_loader, model, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd4236b6bcd4d5d9ea688a3502b2dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data from disk...:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best model and run it on the test set\n",
    "test_dataset = TSPDataset.from_disk(config.data_path, config.num_path_variations_per_example, problem_size_lower_bound=config.test_problem_size_cutoff)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "bssf_checkpoint = config.get_bssf_model_state_dict()\n",
    "bssf_path_const_metric = config.get_bssf_metric()\n",
    "bssf_mse_loss = config.get_bssf_validation_loss()\n",
    "\n",
    "model = ConvolutionalSalesmanNet().to(config.device)\n",
    "model.load_state_dict(bssf_checkpoint)\n",
    "loss_fn = nn.MSELoss().to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c6a592971343c8bbb496a25cce43e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_constructed_paths = []\n",
    "test_mse_loss = 0\n",
    "construction_time_by_size = {}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tot_validation_loss = 0\n",
    "    path_construction_metric = []\n",
    "    for (batch, target) in tqdm(test_loader, \"Testing\"):\n",
    "        batch = batch.squeeze(0).to(config.device)\n",
    "        target = target.squeeze(0).to(config.device)\n",
    "        prob_size = batch.shape[-1]\n",
    "\n",
    "        path_predictions = model(batch)\n",
    "        loss = loss_fn(path_predictions, target)\n",
    "\n",
    "        test_mse_loss += loss.item()\n",
    "\n",
    "        start = time.time()\n",
    "        constructed_path = construct_path(model, batch[0, 0])\n",
    "        end = time.time()\n",
    "\n",
    "        if prob_size not in construction_time_by_size:\n",
    "            construction_time_by_size[prob_size] = []\n",
    "        construction_time_by_size[prob_size].append(end - start)\n",
    "        path_metric = calc_path_metric(constructed_path, target[0], batch[0, 0])\n",
    "        test_constructed_paths.append(path_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# for size in construction_time_by_size.keys():\n",
    "#     construction_time_by_size[size] = np.mean(construction_time_by_size[size])\n",
    "\n",
    "final_stats = {\n",
    "    \"validation_mse\": bssf_mse_loss,\n",
    "    \"validation_path_metric\": bssf_path_const_metric,\n",
    "    \"test_mse\": test_mse_loss / len(test_loader),\n",
    "    \"test_path_metric\": np.mean(test_constructed_paths),\n",
    "    \"construction_time_by_size\": construction_time_by_size\n",
    "}\n",
    "final_stats_path = os.path.join(config.bssf_path, \"final_stats.json\")\n",
    "\n",
    "with open(final_stats_path, \"w+\") as f:\n",
    "    json.dump(final_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
